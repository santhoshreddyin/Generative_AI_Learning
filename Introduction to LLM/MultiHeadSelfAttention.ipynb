{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"A vanilla multi-head masked self-attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward pass for the multi-head masked self-attention layer.\n",
    "\n",
    "        In this exercise, we include lots of print statements and checks to help you\n",
    "        understand the code and the shapes of the tensors. When actually training\n",
    "        such a model you would not log this information to the console.\n",
    "        \"\"\"\n",
    "\n",
    "        # batch size, sequence length (in tokens), embedding dimensionality (n_embd per token)\n",
    "        B, T, C = x.size()\n",
    "        hs = C // self.n_head  # head size\n",
    "\n",
    "        # print some debug information\n",
    "        print(f\"batch size: {B}\")\n",
    "        print(f\"sequence length: {T}\")\n",
    "        print(f\"embedding dimensionality: {C}\")\n",
    "        print(f\"number of heads: {self.n_head}\")\n",
    "        print(f\"head size: {hs}\")\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # resulting dims for k, q, and v are (B, n_head, T, hs)\n",
    "        k = self.key(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "\n",
    "        # === EXERCISE START: IMPLEMENT THE MULTI-HEAD ATTENTION ===\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: multiply q and k_t matrices, then divide by the square root of d_k\n",
    "        print(\"=== Calculate MatrixMultiplication(Q, K_T) / sqrt(d_k) ===\")\n",
    "\n",
    "        k_t = k.transpose(-2, -1)  # what is the shape of k_t?\n",
    "        d_k = k.size(-1)\n",
    "\n",
    "        # Matrix multiplication (hint: not \"*\")\n",
    "        att = <TODO>\n",
    "\n",
    "        print(f\"q.shape: {q.shape}\")\n",
    "        print(f\"k_t.shape: {k_t.shape}\")\n",
    "        print(f\"d_k: {d_k}\")\n",
    "        print(f\"att.shape: {att.shape}\")\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: set the mask fill value to negative infinity\n",
    "        print(\"=== Apply the attention mask ===\")\n",
    "\n",
    "        masked_fill_value = <TODO>\n",
    "\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, masked_fill_value)\n",
    "\n",
    "        # Show the result of applying the mask\n",
    "        print(f\"att: {att}\")\n",
    "\n",
    "        #######################################################################\n",
    "        # TODO: apply softmax\n",
    "        print(\"=== Softmax ===\")\n",
    "\n",
    "        att = F.softmax(att, dim=<TODO>)\n",
    "\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # Show the result of applying the softmax and check that\n",
    "        # the sum of the attention weights in each row is 1\n",
    "        print(f\"att.shape: {att.shape}\")\n",
    "        print(f\"att: {att}\")\n",
    "        print(f\"att.sum(dim=-1): {att.sum(dim=-1)}\")\n",
    "        att_rows_sum_to_one = all(\n",
    "            ((att.sum(dim=-1) - 1.0) ** 2 < 1e-6).flatten().tolist()\n",
    "        )\n",
    "        print(f\"att_rows_sum_to_one: {att_rows_sum_to_one}\")\n",
    "        if not att_rows_sum_to_one:\n",
    "            raise ValueError(\n",
    "                \"Attention weight rows do not sum to 1. Perhaps the softmax dimension or masked_fill_value is not correct?\"\n",
    "            )\n",
    "\n",
    "        ######################################################################\n",
    "        # TODO: multiply att and v matrices\n",
    "        # (B, n_head, T, T) x (B, n_head, T, hs) -> (B, n_head, T, hs)\n",
    "        print(\"=== Calculate final attention ===\")\n",
    "\n",
    "        y = <TODO>\n",
    "\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "\n",
    "        ######################################################################\n",
    "\n",
    "        # === EXERCISE END: IMPLEMENT THE MULTI-HEAD ATTENTION ===\n",
    "\n",
    "        # re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
